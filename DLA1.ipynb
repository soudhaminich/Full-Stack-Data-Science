{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##What is the function of a summation junction of a neuron? What is threshold activation function?\n",
    "\n",
    "In the summation, all features are multiplied by their weights and bias are summed up. (Y=W1X1+W2X2+b). This summed function is applied over an \n",
    "\n",
    "Activation function. The output from this neuron is multiplied with the weight W3 and supplied as input to the output layer.\n",
    "\n",
    "Step Activation function is also called as Binary Step Function as it produces binary output which means only 0 and 1. In this Function we have \n",
    "\n",
    "Threshold value. Where the input is greater than Threshold value it will give output 1 otherwise it is 0. Hence it is also called as Threshold Activation \n",
    "\n",
    "Function. The limitation of this function is it does not allow multi value outputs which means it cannot classify the inputs into several output categories.\n",
    "\n",
    "Leaky ReLU is the extension to the ReLU Activation Function which overcomes the Dying problem of ReLU Function. Dying ReLU problem is becomes \n",
    "\n",
    "inactive for any input provided to it, due to which the performance of Neural Network gets effected. To rectify this problem, we have Leaky ReLU which \n",
    "\n",
    "has small negative slope for Negative input unlike ReLU Activation Function. It has a constant gradient of 0.01. The Limitation of Leaky ReLU is it has \n",
    "\n",
    "linear curve which cannot be used for complex classification.\n",
    "\n",
    "R(z)={\n",
    "   Z     Z> 0\n",
    "\n",
    " αZ     Z<=0\n",
    "\n",
    "}\n",
    "\n",
    "##What is a step function? What is the difference of step function with threshold function?\n",
    "\n",
    "A step function is a function like that used by the original Perceptron. The output is a certain value, A1, if the input sum is above a certain threshold and \n",
    "\n",
    "A0 if the input sum is below a certain threshold. ... These kinds of step activation functions are useful for binary classification schemes.\n",
    "\n",
    "\n",
    "Binary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold \n",
    "\n",
    "neuron is deactivated. In the above graph, the threshold is zero. This activation function can be used in binary classifications as the name suggests, \n",
    "\n",
    "however it can not be used in a situation where you have multiple classes to deal with.\n",
    "\n",
    "In this Function we have Threshold value. Where the input is greater than Threshold value it will give output 1 otherwise it is 0. Hence it is also called as \n",
    "\n",
    "Threshold Activation Function\n",
    "\n",
    "##Explain the McCulloch–Pitts model of neuron.\n",
    "\n",
    "\n",
    "This is simplified model of real neurons, known as Threshold Logic Unit.\n",
    "\n",
    "A set of synapsesc (i.e connections) brings the activations from the other neurons.\n",
    "\n",
    "A processing unit sums the inputs, the applies the non-linear activation funcation (i.e threshold / transfer function).\n",
    "\n",
    "A output line transmit the result to other neurons.\n",
    "\n",
    "In other word, the input to a neuron arrives in the form of signals.The signals build up in the cell. Finally the cells fires(discharges) through the output. \n",
    "\n",
    "The cell can start building up signals again.\n",
    "\n",
    "Functions:\n",
    "\n",
    "The function y=f(X) describes a relationship , an input-ouput mapping rom x to y. threshold or sign function sgn(x) : define as\n",
    "\n",
    "threshold or sign sigmod : define as a smooth (differentiable) form of threshold function\n",
    "\n",
    "Mcculloch pitts neuron Equation:\n",
    "\n",
    "The equation of output of mcculloch pitts neuron as a function of 1 to n input is given as\n",
    "\n",
    "        sum=\n",
    "        \n",
    "The mcculloch pitts neuron model is extremely simplifield model of real biological neurons. some of its missing features includes: non binary inputs-outputs, non linear summation, smooth thresholding , stochastic(non-deterministic) snd tempory information processing.\n",
    "\n",
    "It inspred by Biological Neural network.\n",
    "\n",
    "It allows only Binary Value(0,1).\n",
    "\n",
    "It has threshold function as activation function.\n",
    "\n",
    "It is first mathmatical model of Biological Neuron\n",
    "\n",
    "The first computational model of a neuron was proposed by Warren MuCulloch (neuroscientist) and Walter Pitts (logician) in 1943.\n",
    "\n",
    "This is where it all began..\n",
    "\n",
    "It may be divided into 2 parts. The first part, g takes an input (ahem dendrite ahem), performs an aggregation and based on the aggregated value the \n",
    "\n",
    "second part, f makes a decision.\n",
    "\n",
    "Lets suppose that I want to predict my own decision, whether to watch a random football game or not on TV. The inputs are all boolean i.e., {0,1} and my \n",
    "\n",
    "output variable is also boolean {0: Will watch it, 1: Won’t watch it}.\n",
    "\n",
    "So, x_1 could be isPremierLeagueOn (I like Premier League more)\n",
    "\n",
    "x_2 could be isItAFriendlyGame (I tend to care less about the friendlies)\n",
    "\n",
    "x_3 could be isNotHome (Can’t watch it when I’m running errands. Can I?)\n",
    "\n",
    "x_4 could be isManUnitedPlaying (I am a big Man United fan. GGMU!) and so on.\n",
    "\n",
    "These inputs can either be excitatory or inhibitory. Inhibitory inputs are those that have maximum effect on the decision making irrespective of other \n",
    "\n",
    "inputs i.e., if x_3 is 1 (not home) then my output will always be 0 i.e., the neuron will never fire, so x_3 is an inhibitory input. Excitatory inputs are NOT \n",
    "\n",
    "the ones that will make the neuron fire on their own but they might fire it when combined together. Formally, this is what is going on:\n",
    "\n",
    "We can see that g(x) is just doing a sum of the inputs — a simple aggregation. And theta here is called thresholding parameter. For example, if I always \n",
    "\n",
    "watch the game when the sum turns out to be 2 or more, the theta is 2 here. This is called the Thresholding Logic.\n",
    "\n",
    "##Explain the ADALINE network model.\n",
    "\n",
    "ADALINE:\n",
    "\n",
    "Known as Adaptive Linear Neuron\n",
    "\n",
    "Adaline is a network with a single linear unit\n",
    "\n",
    "The Adaline network is trained using the delta rule\n",
    "\n",
    "1.1  Architecture\n",
    "\n",
    "As already stated Adaline is a single-unit neuron, which receives input from several units and also from one unit, called bias. An Adeline model consists \n",
    "\n",
    "of trainable weights. The inputs are of two values (+1 or -1) and the weights have signs (positive or negative).\n",
    "\n",
    "Initially random weights are assigned. The net input calculated is applied to a quantizer transfer function (possibly activation function) that restores the \n",
    "\n",
    "output to +1 or -1. The Adaline model compares the actual output with the target output and with the bias and the adjusts all the weights.\n",
    "\n",
    "1.2  Training Algorithm\n",
    "\n",
    "The Adaline network training algorithm is as follows:\n",
    "\n",
    "Step0: weights and bias are to be set to some random values but not zero. Set the learning rate parameter α.\n",
    "\n",
    "Step1: perform steps 2-6 when stopping condition is false.\n",
    "\n",
    "Step2: perform steps 3-5 for each bipolar training pair s:t\n",
    "\n",
    "Step3: set activations foe input units i=1 to n.\n",
    "\n",
    "Step4: calculate the net input to the output unit.\n",
    "\n",
    "Step5: update the weight and bias for i=1 to n\n",
    "\n",
    "Step6: if the highest weight change that occurred during training is smaller than a specified tolerance then stop the training process, else continue. This \n",
    "\n",
    "is the test for the stopping condition of a network.\n",
    "\n",
    "1.3  Testing Algorithm\n",
    "\n",
    "It is very essential to perform the testing of a network that has been trained. When the training has been completed, the Adaline can be used to classify \n",
    "\n",
    "input patterns. A step function is used to test the performance of the network. The testing procedure for the Adaline network is as follows:\n",
    "\n",
    "\n",
    "Step0: initialize the weights. (The weights are obtained from the training algorithm.)\n",
    "\n",
    "Step1: perform steps 2-4 for each bipolar input vector x.\n",
    "\n",
    "Step2: set the activations of the input units to x.\n",
    "\n",
    "Step3: calculate the net input to the output units\n",
    "\n",
    "Step4: apply the activation function over the net input calculated.\n",
    "\n",
    "##What is the constraint of a simple perceptron? Why it may fail with a real-world data set?\n",
    "\n",
    "Perceptron networks have several limitations. First, the output values of a perceptron can take on only one of two values (0 or 1) due to the hard-limit \n",
    "\n",
    "transfer function. Second, perceptrons can only classify linearly separable sets of vectors.\n",
    "\n",
    "\n",
    "Anything that is not linearly separable cant be solved perceptrons, unless you use feature maps on data to map them to a higher dimension in which it is \n",
    "\n",
    "linearly separable. As a simple, concrete example, perceptron cant learn the XOR function\n",
    "\n",
    "##What is linearly inseparable problem? What is the role of the hidden layer?\n",
    "\n",
    "Linear inseparability\n",
    "\n",
    "Clearly not all decision problems are linearly separable: they cannot be solved using a linear decision boundary. Problems like these are termed linearly \n",
    "\n",
    "inseparable.\n",
    "\n",
    "A dataset is said to be linearly separable if it is possible to draw a line that can separate the red and green points from each other. Here are same \n",
    "\n",
    "examples of linearly separable data: And here are some examples of linearly non-separable data. This concept can be extended to three or more \n",
    "\n",
    "dimensions as welL\n",
    "\n",
    "First of all, hidden layer in artificial neural networks a layer of neurons, whose output is connected to the inputs of other neurons and therefore is not \n",
    "\n",
    "visible as a network output.\n",
    "\n",
    "Now, let me explain the role of the hidden layers on the following example: There is a well-known problem of facial recognition, where computer learns \n",
    "\n",
    "to detect human faces. Human face is a complex object, it must have eyes, a nose, a mouth, and to be in a round shape, for computer it means that \n",
    "\n",
    "there are a lot of pixels of different colors that are comprised in different shapes. And in order to decide whether there is a human face on a picture, \n",
    "\n",
    "computer has to detect all those objects.\n",
    "\n",
    "##Explain XOR problem in case of a simple perceptron.\n",
    "\n",
    "Perceptron for XOR:\n",
    "\n",
    "XOR is where if one is 1 and other is 0 but not both.\n",
    "\n",
    "Need:\n",
    "1.w1 + 0.w2 cause a fire, i.e. >= t\n",
    "\n",
    "0.w1 + 1.w2 >= t\n",
    "\n",
    "0.w1 + 0.w2 doesn't fire, i.e. < t\n",
    "\n",
    "1.w1 + 1.w2 also doesn't fire, < t\n",
    "\n",
    "w1 >= t\n",
    "\n",
    "w2 >= t\n",
    "\n",
    "0 < t\n",
    "\n",
    "w1+w2 < t\n",
    "\n",
    "Contradiction.\n",
    "\n",
    "Note: We need all 4 inequalities for the contradiction. If weights negative, e.g. weights = -4 and t = -5, then weights can be greater than t yet adding \n",
    "\n",
    "them is less than t, but t > 0 stops this.\n",
    "\n",
    "A \"single-layer\" perceptron can't implement XOR. The reason is because the classes in XOR are not linearly separable. You cannot draw a straight line \n",
    "\n",
    "to separate the points (0,0),(1,1) from the points (0,1),(1,0).\n",
    "\n",
    "Led to invention of multi-layer networks.\n",
    "\n",
    "Since its creation, the perceptron model went through significant modifications. We discovered different activation functions, learning rules and even \n",
    "\n",
    "weight initialization methods. Because of these modifications and the development of computational power, we were able to develop deep neural nets \n",
    "\n",
    "capable of learning non-linear problems significantly more complex than the XOR function. The only caveat with these networks is that their fundamental \n",
    "\n",
    "unit is still a linear classifier. So their representational power comes from their multi-layered structure, their architecture and their size.\n",
    "\n",
    "Trying to improve on that, I’d like to propose an adaptive polynomial transformation in order to increase the representational power of a single artificial \n",
    "\n",
    "neuron. In the next section I’ll quickly describe the original concept of a perceptron and why it wasn’t able to fit the XOR function. I’ll then overview the \n",
    "\n",
    "changes to the perceptron model that were crucial to the development of neural networks. In section 4, I’ll introduce the polynomial transformation and \n",
    "\n",
    "compare it to the linear one while solving logic gates. Finally I’ll comment on what I believe this work demonstrates and how I think future work can \n",
    "\n",
    "explore it.\n",
    "\n",
    "2 - The Perceptron and its Nemesis in the 60s\n",
    "\n",
    "The perceptron is a model of a hypothetical nervous system originally proposed by Frank Rosenblatt in 1958. It was heavily based on previous works \n",
    "\n",
    "from McCullock, Pitts and Hebb, and it can be represented by the schematic shown in the figure below.\n",
    "\n",
    "Image taken from Towards Data Science.\n",
    "\n",
    "As we can see, it calculates a weighted sum of its inputs and thresholds it with a step function. Geometrically, this means the perceptron can separate \n",
    "\n",
    "its input space with a hyperplane. That’s where the notion that a perceptron can only separate linearly separable problems came from. Since the XOR \n",
    "\n",
    "function is not linearly separable, it really is impossible for a single hyperplane to separate it.\n",
    "\n",
    "Tables and graphs adapted from Kevin Swingler.\n",
    "\n",
    "An obvious solution was to stack multiple perceptrons together. Although, there was a problem with that. When Rosenblatt introduced the perceptron, he \n",
    "\n",
    "also introduced the perceptron learning rule(the algorithm used to calculate the correct weights for a perceptron automatically). The rule didn’t \n",
    "\n",
    "generalize well for multi-layered networks of perceptrons, thus making the training process of these machines a lot more complex and, most of the time, \n",
    "\n",
    "an unknown process. This limitation ended up being responsible for a huge disinterest and lack of funding of neural networks research for more than 10 \n",
    "\n",
    "years [reference].\n",
    "\n",
    "##Design a multi-layer perceptron to implement A XOR B.\n",
    "\n",
    "For the XOR gate, the TRUTH table will be as follows\n",
    "\n",
    "XOR truth table\n",
    "\n",
    "XOR is a classification problem, as it renders binary distinct outputs. If we plot the INPUTS vs OUTPUTS for the XOR gate, it would look something like\n",
    "\n",
    "The graph plots the two inputs corresponding to their output. Visualizing this plot, we can see that it is impossible to separate the different outputs (1 and \n",
    "\n",
    "0) using a linear equation.\n",
    "\n",
    "To separate the two outputs using linear equation(s), we would need to draw two separate lines like\n",
    "\n",
    "Image courtesy: https://tex.stackexchange.com/\n",
    "\n",
    "The above graph perfectly shows why these outputs cannot be separated using a single linear equation. This was a major problem with the initial \n",
    "\n",
    "perceptrons (single layer approach).\n",
    "\n",
    "What is the XOR problem?\n",
    "As we have seen above, it is impossible to separate the XOR outputs using just a single linear equation. This is a major problem as during the training \n",
    "\n",
    "of machines, for optimized outputs, the machine is expected to form the mathematical equations on its own.\n",
    "\n",
    "For a problem resembling the outputs of XOR, it was impossible for the machine to set up an equation for good outputs. This is what led to the birth of \n",
    "\n",
    "the concept of hidden layers which are extensively used in Artificial Neural Networks.\n",
    "\n",
    "Let’s call the output to be Y, so\n",
    "\n",
    "Y = A1X1 + A2X2 + A3X3 + …. + B\n",
    "\n",
    "Here B is the bias, and A1, A2, A3 are the weights. Weights are used to control the signal (strength of the connection) of the connection.\n",
    "\n",
    "Y can also be called the weighted sum.\n",
    "\n",
    "The information flow inside a perceptron is a feed-forward type, meaning that the signal flows in a single direction from the input layer to the output \n",
    "\n",
    "layer. All the input layers are independent of each other.\n",
    "\n",
    "The variation in the weight variables controls the process of conversion of the input values to the output values.\n",
    "\n",
    "The main limitation of a single-layer architecture (perceptrons) is that it separates the data points using a single line. This has a drawback in a problem \n",
    "\n",
    "similar to the XOR problem, as the data points are linearly inseparable.\n",
    "\n",
    "How is the XOR problem solved?\n",
    "\n",
    "The solution to the XOR problem lies in multidimensional analysis. We plug in numerous inputs in various layers of interpretation and processing, to \n",
    "\n",
    "generate the optimum outputs.\n",
    "\n",
    "The inner layers for deeper processing of the inputs are known as hidden layers. The hidden layers are not dependent on any other layers. This \n",
    "\n",
    "architecture is known as Multilayer Perceptron (MLP).\n",
    "\n",
    "The layers in a perceptron\n",
    "\n",
    "The number of layers in MLP is not fixed and thus can have any number of hidden layers for processing. In the case of MLP, the weights are defined for \n",
    "\n",
    "each hidden layer, which transfers the signal to the next proceeding layer.\n",
    "\n",
    "Using the MLP approach lets us dive into more than two dimensions, which in turn lets us separate the outputs of XOR using multidimensional \n",
    "\n",
    "equations.\n",
    "\n",
    "Each hidden unit invokes an activation function, to range down their output values to 0 or 1.\n",
    "\n",
    "The MLP approach also lies in the class of feed-forward Artificial Neural Network, and thus can only communicate in one direction. MLP solves the XOR \n",
    "\n",
    "problem efficiently by visualizing the data points in multi-dimensions and thus constructing an n-variable equation to fit in the output values.\n",
    "\n",
    "In this blog, we read about the popular XOR problem and how it is solved by using multi-layered perceptrons. These problems give a sense of \n",
    "\n",
    "understanding of how deep neural networks work to solve complex problems.\n",
    "\n",
    "##Explain the single-layer feed forward architecture of ANN.\n",
    "\n",
    "Single Layer Feed Forward Network\n",
    "\n",
    "In this type of network, we have only two layers, i.e. input layer and output layer but the input layer does not count because no computation is performed \n",
    "\n",
    "in this layer.\n",
    "\n",
    "Output Layer is formed when different weights are applied on input nodes and the cumulative effect per node is taken.\n",
    "\n",
    "After this, the neurons collectively give the output layer to compute the output signals.\n",
    "\n",
    "In this type of network, we have only two layers input layer and output layer but the input layer does not count because no computation is performed in \n",
    "\n",
    "this layer. The output layer is formed when different weights are applied on input nodes and the cumulative effect per node is taken. After this, the \n",
    "\n",
    "neurons collectively give the output layer to compute the output signals.\n",
    "\n",
    "##Explain the competitive network architecture of ANN.\n",
    "\n",
    "Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the \n",
    "\n",
    "input data.[1] A variant of Hebbian learning, competitive learning works by increasing the specialization of each node in the network. It is well suited to \n",
    "\n",
    "finding clusters within data.\n",
    "\n",
    "Models and algorithms based on the principle of competitive learning include vector quantization and self-organizing maps (Kohonen maps).\n",
    "\n",
    "Competitive Learning is usually implemented with Neural Networks that contain a hidden layer which is commonly known as “competitive layer”.[6] \n",
    "\n",
    "Every competitive neuron is described by a vector of weights\n",
    "\n",
    "For every input vector, the competitive neurons “compete” with each other to see which one of them is the most similar to that particular input vector. \n",
    "\n",
    "The winner neuron m sets its output {\\displaystyle o_{m}=1}{\\displaystyle o_{m}=1} and all the other competitive neurons set their output {\\displaystyle \n",
    "\n",
    "o_{i}=0,i=1,..,M,i\\neq m}o_{i}=0,i=1,..,M,i\\neq m.\n",
    "\n",
    "Usually, in order to measure similarity the inverse of the Euclidean distance is used\n",
    "\n",
    "Here is a simple competitive learning algorithm to find three clusters within some input data.\n",
    "\n",
    "1.(Set-up.) Let a set of sensors all feed into three different nodes, so that every node is connected to every sensor. Let the weights that each node gives \n",
    "\n",
    "to its sensors be set randomly between 0.0 and 1.0. Let the output of each node be the sum of all its sensors, each sensor's signal strength being \n",
    "\n",
    "multiplied by its weight.\n",
    "\n",
    "2.When the net is shown an input, the node with the highest output is deemed the winner. The input is classified as being within the cluster \n",
    "\n",
    "corresponding to that node.\n",
    "\n",
    "3.The winner updates each of its weights, moving weight from the connections that gave it weaker signals to the connections that gave it stronger \n",
    "\n",
    "signals.\n",
    "\n",
    "Thus, as more data are received, each node converges on the centre of the cluster that it has come to represent and activates more strongly for inputs \n",
    "\n",
    "in this cluster and more weakly for inputs in other clusters.\n",
    "\n",
    "##Consider a multi-layer feed forward neural network. Enumerate and explain steps in the backpropagation algorithm used to train the network.\n",
    "\n",
    "The Backpropagation algorithm is a supervised learning method for multilayer feed-forward networks from the field of Artificial Neural Networks.\n",
    "\n",
    "Feed-forward neural networks are inspired by the information processing of one or more neural cells, called a neuron. A neuron accepts input signals via \n",
    "\n",
    "its dendrites, which pass the electrical signal down to the cell body. The axon carries the signal out to synapses, which are the connections of a cell’s \n",
    "\n",
    "axon to other cell’s dendrites.\n",
    "\n",
    "The principle of the backpropagation approach is to model a given function by modifying internal weightings of input signals to produce an expected \n",
    "\n",
    "output signal. The system is trained using a supervised learning method, where the error between the system’s output and a known expected output is \n",
    "\n",
    "presented to the system and used to modify its internal state.\n",
    "\n",
    "\n",
    "Technically, the backpropagation algorithm is a method for training the weights in a multilayer feed-forward neural network. As such, it requires a \n",
    "\n",
    "network structure to be defined of one or more layers where one layer is fully connected to the next layer. A standard network structure is one input \n",
    "\n",
    "layer, one hidden layer, and one output layer.\n",
    "\n",
    "Backpropagation can be used for both classification and regression problems, but we will focus on classification in this tutorial.\n",
    "\n",
    "In classification problems, best results are achieved when the network has one neuron in the output layer for each class value. For example, a 2-class \n",
    "\n",
    "or binary classification problem with the class values of A and B. These expected outputs would have to be transformed into binary vectors with one \n",
    "\n",
    "column for each class value. Such as [1, 0] and [0, 1] for A and B respectively. This is called a one hot encoding.\n",
    "\n",
    "##What are the advantages and disadvantages of neural networks?\n",
    "\n",
    "Here are some advantages of Artificial Neural Networks ( ANN)\n",
    "\n",
    "Storing information on the entire network: Information such as in traditional programming is stored on the entire network, not on a database. The \n",
    "\n",
    "disappearance of a few pieces of information in one place does not restrict the network from functioning. \n",
    "\n",
    "The ability to work with inadequate knowledge: After ANN training, the data may produce output even with incomplete information. The lack of \n",
    "\n",
    "performance here depends on the importance of the missing information. \n",
    "\n",
    "It has fault tolerance:  Corruption of one or more cells of ANN does not prevent it from generating output. This feature makes the networks fault-tolerant. \n",
    "\n",
    "Having a distributed memory: For ANN to be able to learn, it is necessary to determine the examples and to teach the network according to the desired \n",
    "\n",
    "output by showing these examples to the network. The network's progress is directly proportional to the selected instances, and if the event can not be \n",
    "\n",
    "shown to the network in all its aspects, the network can produce incorrect output \n",
    "\n",
    "Gradual corruption:  A network slows over time and undergoes relative degradation. The network problem does not immediately corrode.\n",
    "\n",
    "Ability to train machine: Artificial neural networks learn events and make decisions by commenting on similar events. \n",
    "\n",
    " Parallel processing ability:  Artificial neural networks have numerical strength that can perform more than one job at the same time. \n",
    "\n",
    "Disadvantages of Artificial Neural Networks (ANN)\n",
    "\n",
    "Hardware dependence:  Artificial neural networks require processors with parallel processing power, by their structure. For this reason, the realization of \n",
    "\n",
    "the equipment is dependent. \n",
    "\n",
    "Unexplained functioning of the network: This is the most important problem of ANN. When ANN gives a probing solution, it does not give a clue as to \n",
    "\n",
    "why and how. This reduces trust in the network. \n",
    "\n",
    "Assurance of proper network structure:  There is no specific rule for determining the structure of artificial neural networks. The appropriate network \n",
    "\n",
    "structure is achieved through experience and trial and error. \n",
    "\n",
    "The difficulty of showing the problem to the network:  ANNs can work with numerical information. Problems have to be translated into numerical values \n",
    "\n",
    "before being introduced to ANN. The display mechanism to be determined here will directly influence the performance of the network. This depends on \n",
    "\n",
    "the user's ability. \n",
    "\n",
    "The duration of the network is unknown: The network is reduced to a certain value of the error on the sample means that the training has been \n",
    "\n",
    "completed. This value does not give us optimum results. \n",
    "\n",
    "##Write short notes on any two of the following:\n",
    "\n",
    "###1.Biological neuron\n",
    "\n",
    "###2.ReLU function\n",
    "\n",
    "###3.Single-layer feed forward ANN\n",
    "\n",
    "###4.Gradient descent\n",
    "\n",
    "###5.Recurrent networks\n",
    "\n",
    "Biological neural networks tolerate a great deal of ambiguity in data. However, artificial neural networks require somewhat precise, structured, and \n",
    "\n",
    "formatted data to tolerate ambiguity. Biological neural networks are fault-tolerant to a certain level, and the minor failures will not always result in \n",
    "\n",
    "memory loss.\n",
    "\n",
    "The brain can recover and heal to an extent. But the artificial neural networks are not designed for fault tolerance or self-regeneration. We can still \n",
    "\n",
    "sometimes recover by saving the model’s current weight values and continuing the training from the saved state. \n",
    "\n",
    "Talking about power consumption, the brain requires about 20% of all the human body’s energy, equivalent to about 20 watts, which is exceptionally \n",
    "\n",
    "efficient. But computers need an enormous amount of computational power to solve the same problem, and they also generate a lot of heat during \n",
    "\n",
    "computation.\n",
    "\n",
    "Artificial neural networks were inspired by the biological neural networks of the human body. The modeling of biological neural networks was a crucial \n",
    "\n",
    "step in the development of artificial neural networks. Many scientists attempted to understand the working of the brain. Artificial neural networks today \n",
    "\n",
    "are being used for various applications, some are biologically related, and most of them are engineering related.\n",
    "\n",
    "\n",
    "Even though biological neural networks and artificial neural networks are similar in function, they still have many differences. Many attempts have been \n",
    "\n",
    "made to understand the complex mechanism of biological neural networks. Yet, they still hold many secrets to unlock and inspire the future of artificial \n",
    "\n",
    "intelligence.\n",
    "\n",
    "Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. This method is \n",
    "\n",
    "commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear regression). Due to its importance and \n",
    "\n",
    "ease of implementation, this algorithm is usually taught at the beginning of almost all machine learning courses.\n",
    "\n",
    "\n",
    "However, its use is not limited to ML/DL only, it’s being widely used also in areas like:\n",
    "\n",
    "\n",
    "control engineering (robotics, chemical, etc.)\n",
    "\n",
    "computer games\n",
    "\n",
    "mechanical engineering\n",
    "\n",
    "\n",
    "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these \n",
    "\n",
    "models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of \n",
    "\n",
    "parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
