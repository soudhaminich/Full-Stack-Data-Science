{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.\tWhat are Sequence-to-sequence models?\n",
    "\n",
    "Sequence to Sequence (often abbreviated to seq2seq) models is a special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization, etc.\n",
    "\n",
    "##2.\tWhat are the Problem with Vanilla RNNs?\n",
    "\n",
    "However, RNNs suffer from the problem of vanishing gradients, which hampers learning of long data sequences. The gradients carry information used in the RNN parameter update and when the gradient becomes smaller and smaller, the parameter updates become insignificant which means no real learning is done.\n",
    "\n",
    "##3.\tWhat is Gradient clipping?\n",
    "\n",
    "Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks. ... With gradient clipping, pre-determined gradient threshold be introduced, and then gradients norms that exceed this threshold are scaled down to match the norm.\n",
    "\n",
    "##4.\tExplain Attention mechanism\n",
    "\n",
    "The attention mechanism is a part of a neural architecture that enables to dynamically highlight relevant features of the input data, which, in NLP, is typically a sequence of textual elements. It can be applied directly to the raw input or to its higher level representation.\n",
    "\n",
    "##5.\tExplain Conditional random fields (CRFs)\n",
    "\n",
    "Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account.\n",
    "\n",
    "##6.\tExplain self-attention\n",
    "\n",
    "Self Attention, also called intra Attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.\n",
    "\n",
    "##7.\tWhat is Bahdanau Attention?\n",
    "\n",
    "Bahdanau attention mechanism proposed an attention mechanism that learns to align and translate jointly. It is also known as Additive attention as it performs a linear combination of encoder states and the decoder states\n",
    "\n",
    "##8.\tWhat is a Language Model?\n",
    "\n",
    "Language models are used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval, and many other daily tasks\n",
    "\n",
    "A language model learns the probability of word occurrence based on examples of text\n",
    "\n",
    "##9.\tWhat is Multi-Head Attention?\n",
    "\n",
    "Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. ... Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies).\n",
    "\n",
    "##10.\tWhat isBilingual Evaluation Understudy (BLEU)\n",
    "\n",
    "BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score is a number between zero and one that measures the similarity of the machine-translated text to a set of high quality reference translations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
