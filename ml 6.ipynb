{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment_6\n",
    "\n",
    "# 1. In the sense of machine learning, what is a model? What is the best way to train a model?\n",
    "\n",
    "Model is an algorithmic function that has been trained on some data. This model can be used for solving prediction and clustering related problems by using input in the form of independent variables the model used to establish patterns and output results using the same patterns.\n",
    "\n",
    "There are multiple ways to build models and can be filtered down by gauging the advantages and disadvantages against our problem statement. But to put in the most general terms, if the key steps in a machine learning model building are done with utmost care, the model would be the best. This includes good data pre-processing practices, carefully choosing an algorithm over others and more.\n",
    "\n",
    "# 2. In the sense of machine learning, explain the \"No Free Lunch\" theorem.Â¶\n",
    "\n",
    "In the sense of machine learning, No Free Lunch theorem means that under some constraints, performance of all optimization algorithms is similar or identical. In addition to that, no machine learning algorithm is the best. Since no single model is the best than the rest of the model always according to the theorem, we should rely on our knowledge of multiple machine learning algorithms and try a wide variety of algorithms to flush out the most optimal one\n",
    "\n",
    "# 3. Describe the K-fold cross-validation mechanism in detail.\n",
    "\n",
    "In K-fold cross validation, data D is subset into k subsets randomly. Let us assume S1...Sk are the subsets where Sk is the kth randomly split subset of data D.\n",
    "\n",
    "In the first iteration, D-S1 is used for training and S1 for testing the model. When the model has been trained and tested, evaluation can be done, score is noted elsewhere and the trained model is discarded.\n",
    "\n",
    "These k-iterations go on where 1/k subset of D is always set aside for testing the data and D-1/k subsets are used for training, evaluating and discarding the model.\n",
    "\n",
    "At the end of all the iterations, average of all the evaluation scores is taken and used as output.\n",
    "\n",
    "# 4. Describe the bootstrap sampling method. What is the aim of it?\n",
    "\n",
    "Boostrap sampling is a method of sampling in which the repeated sampling is done with replacement using a data D in random draws. Bootstrap sampling is often used to simulate sampling distributions instead of relying on theorems like the Central Limit Theorem.\n",
    "\n",
    "In Machine Learning, it is primarily used in ensemble models like Random Forest classifier. Its aim in Machine Learning use case is to avoid overfitting and improve stability of machine learning algorithms while improving performance.\n",
    "\n",
    "# 5. What is the significance of calculating the Kappa value for a classification model? Demonstrate how to measure the Kappa value of a classification model using a sample collection of results.\n",
    "\n",
    "Kappa value or Cohen's Kappa coefficient is an evaluation metric for classification models. Its significance as an evaluation metric is that it can be used to evaluate multi class classification models and also works on models trained on imbalanced datasets(scores like accuracy scores fail for imbalanced datasets).\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "Observed=['0','1','0','1','0','1','0','1','0','1']\n",
    "Predicted=['0','0','1','1','0','0','1','1','0','0']\n",
    "\n",
    "cohen_kappa_score(Observed,Predicted) #True Value\n",
    "\n",
    "agreed=(3+2)/10\n",
    "pyes=((3+2)/10)*((3+2)/10)\n",
    "pno=((3+2)/10)*((3+2)/10)\n",
    "print(agreed,pyes,pno)\n",
    "\n",
    "non_disagreed= pyes+pno\n",
    "print(1-non_disagreed)\n",
    "print(1-agreed)\n",
    "\n",
    "K= 1-(0.5/0.5)\n",
    "K #Calculated Value\n",
    "\n",
    "# 6. Describe the model ensemble method. In machine learning, what part does it play?\n",
    "\n",
    "# Ensemble methods or ensemble machine learning models are models where more than one models are being used spontaneously to produce better results than individually trained models. Ensemble models are of 4 types\n",
    "\n",
    "Bagging which is also called Bootstrap aggregation. Multiple base learners learn on subsets of data. A majority vote based on the results of the individual base learners is then used to output the final result. Example: Random Forest\n",
    "\n",
    "Boosting is mainly used to reduce bias in model. Multiple base learners are trained in a sequence in which each learner covers up the weakness of the previous learner to ultimately produce a powerful model. Example: Adaptive Boosting or Adaboost\n",
    "\n",
    "Stacking is an ensemble model that combines multiple base learner models constructed parallel to each other and independent. All the models produce predictions that are used creating a meta classifier which is trained on the predictions made by base models. The resultant prediction is then taken as the final prediction. Space and Time complexity issue is huge for stacking models.\n",
    "\n",
    "Cascading models are built when the cost of making a mistake in prediction is high. A threshold is chosen for multiple base learners. If one base learner is sure about its prediction(Probability>0.99), then the solution of this learner is taken as the final solution. Else, the problem is sent to a more sophisticated, complex model that does the same as the previous model. The problem either gets solved with probability>0.99 or it keeps getting transfered to a more complicated model that is bound to solve it.\n",
    "\n",
    "# 7. What is a descriptive model's main purpose? Give examples of real-world problems that descriptive models were used to solve.\n",
    "\n",
    "Main purpose of descriptive models is to find patterns and underlying trends. In machine learning, this is done mostly using unsupervised machine learning algorithms.\n",
    "\n",
    "Market analysis based on consumer's purchase data, Social media post engagement analysis and sales data are real world applications of descriptive models.\n",
    "\n",
    "# 8. Describe how to evaluate a linear regression model.\n",
    "\n",
    "Evaluation of a linear regression model can be done using R-square. R square is calculated as the sum of squared errors in predictions made, divided by summation of all sum of squares. R square measures how much of the change in target variable can be explained by the linear regressor. Its value ranges from 0 to 1 where 0 means poor performance and 1 means good.\n",
    "\n",
    "# 9. Distinguish :\n",
    "\n",
    "# i. Descriptive vs. predictive models\n",
    "\n",
    "\n",
    "1. Descriptive models are built to identify trends and underlying patterns. Predictive models are built to predict a dependent variable value.\n",
    " \n",
    " 2. Most of descriptive models are built using unsupervised machine learning. Most of predictive models are built using classification and regression models.\n",
    " \n",
    " 3. Example for descriptive model: Finding why consumers are engaging more with a social media post. Example for predictive model: Predicting the chances of cancer in a patient.\n",
    "\n",
    "# ii. Underfitting vs. overfitting the model\n",
    "\n",
    "\n",
    " 1. Underfitting is a situation arising when the hypothesis is way too simple, or when the machine learning model is way too simple to produce good results. Overfitting is a situation arising when the hypothesis is way too complex, or when the machine learning model is way too complex to produce good results.\n",
    " \n",
    " 2. Underfitting causes a model to produce poor results due to heavily simplified algorithm reacting lightly to changes in the unseen data for independent variables from the training data . Overfitting makes a model produce poor results due to slightest variations in the unseen data for independent variables from the training data \n",
    " \n",
    " 3. Underfitting is also called High Bias. Overfitting is also called High variance.\n",
    "\n",
    "# iii. Bootstrapping vs. cross-validation\n",
    "\n",
    "1. Boostrap sampling is a method of sampling in which the repeated sampling is done with replacement using a data D in random draws over which machine learning models are trained for better performance. Cross validation is a method used to check the efficacy of the machine learning model on test data.\n",
    " \n",
    " 2. End goal of bootstrapping is to reduce overfitting and increase performance. End goal of cross validation is only to produce test scores to check efficacy of model\n",
    "\n",
    " 3. Bootstrapping is best employed in Random Forest Classifier. Cross Validation is best employed using K-fold cross validation technique.\n",
    "\n",
    "# 10. Make quick notes on:\n",
    "\n",
    "# i. LOOCV\n",
    "\n",
    "LOOCV or Leave One Out Cross Validation is a form of K-fold cross validation where only one observation is left out for validation purpose while the rest of the data is used for model training each iteration. It is computationally taxing and should only be used for data with low dimensionality.\n",
    "\n",
    "# ii. F-measurement\n",
    "\n",
    "Harmonic mean of Precision score and recall score is called F-measurement or F-score. It is formulated as 2 * (pr * re)/pr +re where pr is precision score and re is recall score.\n",
    "\n",
    "# iii. The width of the silhouette\n",
    "\n",
    "Estimate of average inter cluster distance to give efficacy/performance of cluster algorithms is called width of the silhouette. It can also be defined as how identical/similar a data point 'x' is to the data points inside the cluster to which x is assigned. Its value ranges from -1 to 1 where 1 means good and -1 means bad.\n",
    "\n",
    "# iv. Receiver operating characteristic curve\n",
    "\n",
    "Curve plotted between True Positive Rate and False Positive Rate is Receiver Operating Characteristics curve and is used to find the area under the curve for ROC-AUC score for binary classification evaluation. True Positive Rate and False Positive Rate are calculated for different thresholds values where thresholds take values starting from the highest probability scores assigned to data points and goes up to the lowest probability score. The curve is impacted by presence of outliers, and simple models. Extensions can be made to this curve to suit multiclass classification evaluation requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
